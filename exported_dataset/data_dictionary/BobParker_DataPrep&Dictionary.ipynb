{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ec8fa2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/bobby/OneDrive/Desktop/Chemical_DataPull_12272022.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12236/374920747.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mDFPath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:/Users/bobby/OneDrive/Desktop/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mfile_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDFPath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'Chemical_DataPull_12272022.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0mfile_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDFPath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'Bacterial_DataPull_12272022.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mfile_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDFPath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'StreamHabitat_DataPull_12272022.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/bobby/OneDrive/Desktop/Chemical_DataPull_12272022.csv'"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "#### Python Code demonstrates benefit of stacking the raw data,       ####\n",
    "#### supplied by Louis Kyphen, in retaining most of the observations, ####\n",
    "#### rather than removing mostly empty columns. Restructering data    ####\n",
    "#### also facilitates focus on specific sampling tests.               #### \n",
    "#### THe code also shows use of Data Dictionary test ranges,          ####\n",
    "#### for purposes of data quality control.                            ####\n",
    "##########################################################################\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "%matplotlib inline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "##### Following function uploads the four data files provided by Louis Kyphen,\n",
    "##### and joins them on the field EventId.\n",
    "##### User of this code assigns a string to the DFPath string variable,\n",
    "##### make sure the data file names are the original names and files are csv.\n",
    "##### Note all lines outputing csv files are commented out throughout the code.\n",
    "#############################################################################\n",
    "\n",
    "DFPath = 'C:/Users/bobby/OneDrive/Desktop/'\n",
    "\n",
    "file_1 = pd.read_csv(DFPath + 'Chemical_DataPull_12272022.csv')\n",
    "file_2 = pd.read_csv(DFPath + 'Bacterial_DataPull_12272022.csv')\n",
    "file_3 = pd.read_csv(DFPath + 'StreamHabitat_DataPull_12272022.csv')\n",
    "file_4 = pd.read_csv(DFPath + 'Macroinverterbrate_DataPull_12272022.csv')\n",
    "\n",
    "\n",
    "def JoinData(DFPath,file_1,file_2,file_3,file_4):\n",
    "    dataset_left = file_1\n",
    "    dataset_right = file_2\n",
    "    dataset_left.merge(file_2, on= ['EventID','EventDate', 'GroupID','GroupName','site_latitude',\\\n",
    "    'site_longitude','site_altitude','SiteID','SiteName'], how='outer')\n",
    "    dataset_right = file_3 \n",
    "    dataset_left.merge(dataset_right, on= ['EventID','EventDate', 'GroupID','GroupName','site_latitude',\\\n",
    "    'site_longitude','site_altitude','SiteID','SiteName'], how='outer')\n",
    "    dataset_right = file_4\n",
    "    data_out = dataset_left.merge(dataset_right, on= ['EventID','EventDate', 'GroupID','GroupName','site_latitude',\\\n",
    "    'site_longitude','site_altitude','SiteID','SiteName'], how='outer')\n",
    "    return(data_out)\n",
    "\n",
    "\n",
    "data_all = JoinData(DFPath,file_1,file_2,file_3,file_4)\n",
    "data_all.head()\n",
    "data_all.describe()\n",
    "data_all.shape\n",
    "\n",
    "##### data_all.to_csv(DFPath + 'data_all3.csv')\n",
    "\n",
    "\n",
    "##################################################################\n",
    "##### Prepping the data, keep most of the data, rather than discarding it\n",
    "##### Will remove all columns with percent missing = .9995 columns have least 36 values.\n",
    "##### For remaining columns, this provides a reasonable minimum sample count in each column.\n",
    "##################################################################\n",
    "\n",
    "\n",
    "data_all.isnull().any().any()\n",
    "x = data_all.isnull().sum() \n",
    "y = x[x > 0]\n",
    "null_percent = y/data_all.shape[0]\n",
    "null_percent\n",
    "\n",
    "\n",
    "missing_features = null_percent[null_percent > 0.9995].index\n",
    "data_all.drop(missing_features, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "print('Dropped columns:', missing_features)\n",
    "\n",
    "\n",
    "#### next line of code outputs data \n",
    "#### data_all.to_csv(DFPath + 'df_clean.csv')\n",
    "\n",
    "data_all.shape\n",
    "data_all.describe()\n",
    "data_all.head()\n",
    "\n",
    "#######################################################################################\n",
    "#### Stacking Sample Data serves to separate individual sample tests.\n",
    "#### Following code stacks the data using Python melt method, \n",
    "#### Other methods such as stack and pivot accomplish similar data restructuring.\n",
    "#### id_vars variables appear on the each row of the stacked data, and all other columns get stacked.\n",
    "#### Of the 72,261 unique EventIDs,from the combined data file,\n",
    "#### stacking and deleting rows with no values keeps 71,474 unique eventIDs.\n",
    "#### Output file for stacking has about 500K rows.\n",
    "######################################################################################\n",
    "\n",
    "df_melt = data_all\n",
    "df_melt = df_melt.melt(id_vars =[\"EventID\", \"EventDate\", \"GroupID\", \"GroupName\", \"SiteID\", \"SiteName\", \\\n",
    "        \"site_latitude\", \"site_longitude\", \"site_altitude\"]).dropna().reset_index()\n",
    "\n",
    "df_melt.shape\n",
    "\n",
    "df_melt.head(10)\n",
    "\n",
    "df_melt.describe()\n",
    "\n",
    "### df_melt.to_csv(DFPath + 'df_melt.csv')\n",
    "\n",
    "df_melt.groupby('EventID').nunique()\n",
    "data_all.groupby('EventID').nunique()\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "##### Many of the values are string type, especially for the Macrointerverbrate measurement columns.\n",
    "##### Additionally, the data has multiple comments, which are string values\n",
    "##### It is sensible to break the full data file into two files, one for the numeric values, and another,\n",
    "##### for the categorical values. Remaining code deals with only numerical values, \n",
    "##### and using the Data Dictionary to classify the values as plausible and/or typical.\n",
    "##### Output file has new 2 binary columns, showing the value column (the measurement) \n",
    "##### as plausible and/or typical. Note range values in the g(x) can be changed easily.\n",
    "##########################################################################################\n",
    "\n",
    "df_num = df_melt[pd.to_numeric(df_melt.value, errors= 'coerce').notnull()]\n",
    "\n",
    "### df_num.to_csv(DFPath + 'df_num.csv')\n",
    "\n",
    "\n",
    "df_num['value'] = pd.to_numeric(df_num['value'], errors='coerce')\n",
    "\n",
    "def f(x):\n",
    "  if x[\"value\"] < 0: return 0\n",
    "  else: return 1\n",
    "\n",
    "df_num[\"plausible_values\"] = df_num.apply(f, axis=1)\n",
    "\n",
    "\n",
    "def g(x):\n",
    "  if x[\"variable\"] == \"DissolvedOxygen1\" and 5 <= x[\"value\"]<= 12 \\\n",
    "  or x[\"variable\"] == \"DissolvedOxygen2\" and 5 <= x[\"value\"]<= 12 \\\n",
    "  or x[\"variable\"] == \"ph1\" and 0 <= x[\"value\"]<= 14 \\\n",
    "  or x[\"variable\"] == \"ph2\" and 0 <= x[\"value\"]<= 14 \\\n",
    "  or x[\"variable\"] == \"Conductivity\" and 0 <= x[\"value\"]<= 10000 \\\n",
    "  or x[\"variable\"] == \"SecchiDisk1\" and 0 <= x[\"value\"]<= 10 \\\n",
    "  or x[\"variable\"] == \"SecchiDisk2\" and 0 <= x[\"value\"]<= 10 \\\n",
    "  or x[\"variable\"] == \"do_saturation\" and 80 <= x[\"value\"]<= 120 \\\n",
    "  or x[\"variable\"] == \"Turbidity\" and 0 <= x[\"value\"]<= 1000 \\\n",
    "  or x[\"variable\"] == \"Salinity1\" and 0 <= x[\"value\"]<= 35 \\\n",
    "  or x[\"variable\"] == \"Salinity2\" and 0 <= x[\"value\"]<= 35 \\\n",
    "  or x[\"variable\"] == \"Alkaliniy\" and 20 <= x[\"value\"]<= 200 \\\n",
    "  or x[\"variable\"] == \"Orthophosphate\" and 1 <= x[\"value\"]<= 3.5 \\\n",
    "  or x[\"variable\"] == \"NitrateN\" and 0 <= x[\"value\"]<= 10 \\\n",
    "  : return 1\n",
    "  else: return 0\n",
    "\n",
    "df_num[\"typical_values\"] = df_num.apply(g, axis=1)\n",
    "\n",
    "##### must delete any empty rows appearing in output csv file\n",
    "\n",
    "df_num.replace('', np.nan, inplace=True)\n",
    "df_num.dropna(inplace=True)\n",
    "#### df_num.to_csv(DFPath + 'df_numrev2.csv')\n",
    "\n",
    "df_num.shape\n",
    "df_num.describe()\n",
    "df_num.head()\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "##### Basic chart shows typical(1)/nontypical(0) and plausible (1)/ not plausible(0)\n",
    "##### Most values are plausible, however many are not typical, \n",
    "##### based on data dictionary ranges shown in the Python code.\n",
    "###################################################################################\n",
    "\n",
    "sns.countplot(df_num['plausible_values'])\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(df_num['typical_values'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#### select all values which are plausible and continue with further work in\n",
    "#### basic data analysis\n",
    "\n",
    "df_pl = df_num[df_num['plausible_values'] > 0]\n",
    "df_pl.head()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "chart = df_pl['variable'].value_counts().plot(kind='bar')\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "####  Following function adds two columns useful for attribute analysis:\n",
    "####  a count for siteName attached to each row, as well as a count of observations for each type \n",
    "####  chemical or biological test recorded\n",
    "##############################################################################################\n",
    "\n",
    "\n",
    "def AddCountVar(df_pl,countvar1,namecount):\n",
    "    newdata1 = df_pl.groupby([countvar1]).size().sort_values(ascending=False).reset_index(name = namecount)\n",
    "    newdata2 = pd.merge(df_pl, newdata1)\n",
    "    return(newdata2)\n",
    "\n",
    "df_obs = AddCountVar(df_pl,'variable','countObs')\n",
    "\n",
    "df_obs2 = AddCountVar(df_obs,'SiteName','countSites')\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "#### df_obs2 is the original data observations with added columns for\n",
    "#### typical values, plausible values, count of observations by type of sampling done\n",
    "#### and count of observations made at each SiteName\n",
    "####################################################################################\n",
    "\n",
    "#### df_obs2.to_csv(DFPath + 'df_obs2.csv')\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "##### Draw histogram showing SiteName Counts to allow subsetting the data. \n",
    "##### Task here is to chart SiteNames with highest counts of observations.\n",
    "##### Bins provide amount detail to allow user to cut the data.\n",
    "######################################################################################\n",
    "\n",
    "\n",
    "plt.hist(df_obs2['countSites'],color = 'blue', edgecolor = 'black', bins = 30)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "df_obs3 = df_obs2[df_obs2['countSites'] > 2000]\n",
    "\n",
    "##### Here, we select all SiteNames with a total of 2000 observations to chart\n",
    "\n",
    "df_obs3.shape\n",
    "\n",
    "chart = df_obs3['SiteName'].value_counts().plot(kind='bar')\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "\n",
    "########################################################################################\n",
    "##### As a final task in the notebook, we draw chart box plots of \n",
    "##### DissolvedOxygen1 to show which sites may be outliers for mean measure\n",
    "##### More extensive data analysis using ANOVA methods can be used for data quality control\n",
    "##### In future notebooks.\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "df_obs4 = df_obs3[df_obs3['variable'] == 'DissolvedOxygen1']\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "ax = sns.boxplot(x='SiteName', y='value', data=df_obs4, color='#99c2a2')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45);\n",
    "\n",
    "########################################################################################\n",
    "### SiteName Boot Harbor appears as outlier for DissolvedOxygen1 test, several others appear\n",
    "### More definitive statistics tests including defined outlier SiteNames can be confirmed with ANOVA.\n",
    "### ####################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512c6ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
